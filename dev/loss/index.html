<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Loss Function ¬∑ UMAP.jl Documentation</title><meta name="title" content="Loss Function ¬∑ UMAP.jl Documentation"/><meta property="og:title" content="Loss Function ¬∑ UMAP.jl Documentation"/><meta property="twitter:title" content="Loss Function ¬∑ UMAP.jl Documentation"/><meta name="description" content="Documentation for UMAP.jl Documentation."/><meta property="og:description" content="Documentation for UMAP.jl Documentation."/><meta property="twitter:description" content="Documentation for UMAP.jl Documentation."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">UMAP.jl Documentation</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Examples</span><ul><li><a class="tocitem" href="../examples/mnist/">MNIST</a></li><li><a class="tocitem" href="../examples/fashion_mnist/">Fashion MNIST</a></li></ul></li><li><span class="tocitem">Usage</span><ul><li><a class="tocitem" href="../examples/advanced_usage/">Advanced Usage</a></li><li><a class="tocitem" href="../examples/composite/">Composite Views</a></li></ul></li><li class="is-active"><a class="tocitem" href>Loss Function</a><ul class="internal"><li><a class="tocitem" href="#Fuzzy-Set-Cross-Entropy"><span>Fuzzy Set Cross Entropy</span></a></li><li><a class="tocitem" href="#Generalization-to-\\ell-Skeleta"><span>Generalization to <span>$\ell$</span>-Skeleta</span></a></li><li><a class="tocitem" href="#Simplified-Loss-for-Optimization"><span>Simplified Loss for Optimization</span></a></li><li><a class="tocitem" href="#Stochastic-Sampling"><span>Stochastic Sampling</span></a></li><li><a class="tocitem" href="#Differentiable-Membership-Function"><span>Differentiable Membership Function</span></a></li><li><a class="tocitem" href="#Optimization-Algorithm"><span>Optimization Algorithm</span></a></li></ul></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../ref/public/">Public API</a></li><li><a class="tocitem" href="../ref/internal/">Internal</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Loss Function</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Loss Function</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/dillondaudert/UMAP.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ÔÇõ</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/dillondaudert/UMAP.jl/blob/master/docs/src/loss.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ÔÅÑ</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="UMAP-Loss-Function"><a class="docs-heading-anchor" href="#UMAP-Loss-Function">UMAP Loss Function</a><a id="UMAP-Loss-Function-1"></a><a class="docs-heading-anchor-permalink" href="#UMAP-Loss-Function" title="Permalink"></a></h1><p>This page describes the fuzzy set cross entropy loss function used in UMAP and how it is optimized during the embedding process.</p><h2 id="Fuzzy-Set-Cross-Entropy"><a class="docs-heading-anchor" href="#Fuzzy-Set-Cross-Entropy">Fuzzy Set Cross Entropy</a><a id="Fuzzy-Set-Cross-Entropy-1"></a><a class="docs-heading-anchor-permalink" href="#Fuzzy-Set-Cross-Entropy" title="Permalink"></a></h2><p>Given two fuzzy simplicial sets, we can consider the 1-skeleta as a fuzzy graph (i.e., a set of edges, where each edge has a probability of existing in the graph). The two sets (of edges) can be compared by computing the set cross entropy.</p><p>For a set <span>$A$</span> and membership functions <span>$\mu: A \to [0, 1]$</span> and <span>$\nu: A \to [0, 1]$</span>, the set cross entropy is:</p><p class="math-container">\[C(A, \mu, \nu) = \sum_{a \in A} \left[ \mu(a) \log\frac{\mu(a)}{\nu(a)} + (1 - \mu(a)) \log\frac{1 - \mu(a)}{1 - \nu(a)} \right]\]</p><p>In code:</p><pre><code class="language-julia hljs">function cross_entropy(A::Set, Œº, ŒΩ)
    loss = 0
    for a ‚àà A
        loss += Œº(a) * log(Œº(a) / ŒΩ(a)) + (1 - Œº(a)) * log((1 - Œº(a)) / (1 - ŒΩ(a)))
    end
    return loss
end</code></pre><h2 id="Generalization-to-\\ell-Skeleta"><a class="docs-heading-anchor" href="#Generalization-to-\\ell-Skeleta">Generalization to <span>$\ell$</span>-Skeleta</a><a id="Generalization-to-\\ell-Skeleta-1"></a><a class="docs-heading-anchor-permalink" href="#Generalization-to-\\ell-Skeleta" title="Permalink"></a></h2><p>The loss function can be generalized to <span>$\ell$</span>-skeleta by the weighted sum of the set cross entropies of the fuzzy sets of <span>$i$</span>-simplices. That is,</p><p class="math-container">\[C_\ell(X, Y) = \sum_{i=1}^{\ell} \lambda_i \cdot C(X_i, Y_i)\]</p><p>where <span>$X_i$</span> denotes the <span>$i$</span>-simplices of <span>$X$</span>.</p><p>In code:</p><pre><code class="language-julia hljs">function cross_entropy(ùêÄ::Vector{Set}, Œº, ŒΩ)
    loss = 0
    for A in ùêÄ
        loss += cross_entropy(A, Œº, ŒΩ)
    end
end</code></pre><h2 id="Simplified-Loss-for-Optimization"><a class="docs-heading-anchor" href="#Simplified-Loss-for-Optimization">Simplified Loss for Optimization</a><a id="Simplified-Loss-for-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Simplified-Loss-for-Optimization" title="Permalink"></a></h2><p>During optimization, we can simplify the loss function to only consider terms that aren&#39;t fixed values and minimize that:</p><p class="math-container">\[C(A, \mu, \nu) = -\sum_{a \in A} \left[ \mu(a) \log\nu(a) + (1 - \mu(a)) \log(1 - \nu(a)) \right]\]</p><p>In code:</p><pre><code class="language-julia hljs">function cross_entropy(A::Set, Œº, ŒΩ)
    loss = 0
    for a ‚àà A
        loss += Œº(a) * log(ŒΩ(a)) + (1 - Œº(a)) * log(1 - ŒΩ(a))
    end
    return -loss
end</code></pre><h2 id="Stochastic-Sampling"><a class="docs-heading-anchor" href="#Stochastic-Sampling">Stochastic Sampling</a><a id="Stochastic-Sampling-1"></a><a class="docs-heading-anchor-permalink" href="#Stochastic-Sampling" title="Permalink"></a></h2><p>Instead of calculating the loss over the entire set (if our set is comprised of the 1-simplices, then calculating this loss would have time complexity <span>$\mathcal{O}(n^2)$</span>), we can sample elements with probability <span>$\mu(a)$</span> and update according to the value <span>$\nu(a)$</span>. This takes care of the <span>$\mu(a) \log\nu(a)$</span> term.</p><p>For the negative samples, elements are sampled uniformly and assumed to have <span>$\mu(a) = 0$</span>. This results in a sampling distribution of</p><p class="math-container">\[P(x_i) = \frac{\sum_{a \in A \mid d_0(a) = x_i}(1 - \mu(a))}{\sum_{b \in A \mid d_0(b) \neq x_i}(1 - \mu(b))}\]</p><p>which is approximately uniform for sufficiently large datasets.</p><h2 id="Differentiable-Membership-Function"><a class="docs-heading-anchor" href="#Differentiable-Membership-Function">Differentiable Membership Function</a><a id="Differentiable-Membership-Function-1"></a><a class="docs-heading-anchor-permalink" href="#Differentiable-Membership-Function" title="Permalink"></a></h2><p>To optimize this loss with gradient descent, <span>$\nu(v)$</span> must be differentiable. A smooth approximation for the membership strength of a 1-simplex between two points <span>$x, y$</span> can be given by the following, with dissimilarity function <span>$\sigma$</span> and constants <span>$a$</span>, <span>$b$</span>:</p><p class="math-container">\[\phi(x, y) = \left(1 + a \cdot \sigma(x, y)^{2b}\right)^{-1}\]</p><p>In code:</p><pre><code class="language-julia hljs">œï(x, y, œÉ, a, b) = (1 + a*(œÉ(x, y))^(2b))^(-1)</code></pre><p>The approximation parameters <span>$a$</span>, <span>$b$</span> are chosen by non-linear least squares fitting of the following function <span>$\psi$</span>:</p><p class="math-container">\[\psi(x, y) = \begin{cases}
1 &amp; \text{if } \sigma(x, y) \leq \text{min\_dist} \\
e^{-(\sigma(x, y) - \text{min\_dist})} &amp; \text{otherwise}
\end{cases}\]</p><p>In code:</p><pre><code class="language-julia hljs">œà(x, y, œÉ, min_dist) = œÉ(x, y) ‚â§ min_dist ? 1 : exp(-(œÉ(x, y) - min_dist))</code></pre><h2 id="Optimization-Algorithm"><a class="docs-heading-anchor" href="#Optimization-Algorithm">Optimization Algorithm</a><a id="Optimization-Algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-Algorithm" title="Permalink"></a></h2><p>Optimizing the embedding is accomplished by stochastic gradient descent, where:</p><ul><li><code>fs_set</code> is the set of <span>$\ell$</span>-simplices (typically 1-simplices)</li><li><code>Y_emb</code> is the target embedding of the points that make up the vertices of <code>fs_set</code></li><li><p class="math-container">\[\sigma\]</p>is a differentiable distance measure between points in <code>Y_emb</code></li><li><p class="math-container">\[\phi\]</p>is the differentiable approximation to the fuzzy set membership function for the simplices in the target embedding</li></ul><p>The algorithm proceeds as follows:</p><pre><code class="language-julia hljs">function optimize_embedding(fs_set, Y_emb, œÉ, œï, n_epochs, n_neg_samples)
    Œ∑ = 1  # learning rate
    ‚àálogœï(x, y) = gradient((_x, _y) -&gt; log(œï(_x, _y, œÉ)), x, y)
    ‚àálog1_œï(x, y) = gradient((_x, _y) -&gt; log(1 - œï(_x, _y, œÉ)), x, y)

    for e in 1:n_epochs
        for (a, b, p) in fs_set‚ÇÅ  # iterate over 1-simplices
            if rand() ‚â§ p  # sample with probability p = Œº(a)
                # Attractive force (positive sample)
                ‚àÇa, ‚àÇb = Œ∑ * ‚àálogœï(Y_emb[a], Y_emb[b])
                Y_emb[a] -= ‚àÇa

                # Repulsive forces (negative samples)
                for _ in 1:n_neg_samples
                    c = sample(Y_emb)
                    ‚àÇa, ‚àÇc = Œ∑ * ‚àálog1_œï(Y_emb[a], Y_emb[c])
                    Y_emb[a] -= ‚àÇa
                end
            end
        end
        Œ∑ = 1 - e/n_epochs  # linear learning rate decay
    end

    return Y_emb
end</code></pre><p>The algorithm iterates over edges in the fuzzy simplicial set, sampling each edge with probability equal to its membership strength <span>$\mu(a)$</span>. For each sampled edge:</p><ol><li><strong>Attractive force</strong>: Apply gradient descent on <span>$\log\phi(x, y)$</span> to pull connected points together</li><li><strong>Repulsive forces</strong>: Sample <code>n_neg_samples</code> random points and apply gradient descent on <span>$\log(1 - \phi(x, y))$</span> to push disconnected points apart</li></ol><p>The learning rate <span>$\eta$</span> decays linearly from 1 to 0 over the course of training.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/composite/">¬´ Composite Views</a><a class="docs-footer-nextpage" href="../ref/public/">Public API ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Thursday 8 January 2026 21:20">Thursday 8 January 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
