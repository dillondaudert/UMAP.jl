# UMAP Loss Function

This page describes the fuzzy set cross entropy loss function used in UMAP and how it is optimized during the embedding process.

## Fuzzy Set Cross Entropy

Given two fuzzy simplicial sets, we can consider the 1-skeleta as a fuzzy graph (i.e., a set of edges, where each edge has a probability of existing in the graph). The two sets (of edges) can be compared by computing the set cross entropy.

For a set $A$ and membership functions $\mu: A \to [0, 1]$ and $\nu: A \to [0, 1]$, the set cross entropy is:

```math
C(A, \mu, \nu) = \sum_{a \in A} \left[ \mu(a) \log\frac{\mu(a)}{\nu(a)} + (1 - \mu(a)) \log\frac{1 - \mu(a)}{1 - \nu(a)} \right]
```

In code:
```julia
function cross_entropy(A::Set, Î¼, Î½)
    loss = 0
    for a âˆˆ A
        loss += Î¼(a) * log(Î¼(a) / Î½(a)) + (1 - Î¼(a)) * log((1 - Î¼(a)) / (1 - Î½(a)))
    end
    return loss
end
```

## Generalization to $\ell$-Skeleta

The loss function can be generalized to $\ell$-skeleta by the weighted sum of the set cross entropies of the fuzzy sets of $i$-simplices. That is,

```math
C_\ell(X, Y) = \sum_{i=1}^{\ell} \lambda_i \cdot C(X_i, Y_i)
```

where $X_i$ denotes the $i$-simplices of $X$.

In code:
```julia
function cross_entropy(ğ€::Vector{Set}, Î¼, Î½)
    loss = 0
    for A in ğ€
        loss += cross_entropy(A, Î¼, Î½)
    end
end
```

## Simplified Loss for Optimization

During optimization, we can simplify the loss function to only consider terms that aren't fixed values and minimize that:

```math
C(A, \mu, \nu) = -\sum_{a \in A} \left[ \mu(a) \log\nu(a) + (1 - \mu(a)) \log(1 - \nu(a)) \right]
```

In code:
```julia
function cross_entropy(A::Set, Î¼, Î½)
    loss = 0
    for a âˆˆ A
        loss += Î¼(a) * log(Î½(a)) + (1 - Î¼(a)) * log(1 - Î½(a))
    end
    return -loss
end
```

## Stochastic Sampling

Instead of calculating the loss over the entire set (if our set is comprised of the 1-simplices, then calculating this loss would have time complexity $\mathcal{O}(n^2)$), we can sample elements with probability $\mu(a)$ and update according to the value $\nu(a)$. This takes care of the $\mu(a) \log\nu(a)$ term.

For the negative samples, elements are sampled uniformly and assumed to have $\mu(a) = 0$. This results in a sampling distribution of

```math
P(x_i) = \frac{\sum_{a \in A \mid d_0(a) = x_i}(1 - \mu(a))}{\sum_{b \in A \mid d_0(b) \neq x_i}(1 - \mu(b))}
```

which is approximately uniform for sufficiently large datasets.

## Differentiable Membership Function

To optimize this loss with gradient descent, $\nu(v)$ must be differentiable. A smooth approximation for the membership strength of a 1-simplex between two points $x, y$ can be given by the following, with dissimilarity function $\sigma$ and constants $a$, $b$:

```math
\phi(x, y) = \left(1 + a \cdot \sigma(x, y)^{2b}\right)^{-1}
```

In code:
```julia
Ï•(x, y, Ïƒ, a, b) = (1 + a*(Ïƒ(x, y))^(2b))^(-1)
```

The approximation parameters $a$, $b$ are chosen by non-linear least squares fitting of the following function $\psi$:

```math
\psi(x, y) = \begin{cases}
1 & \text{if } \sigma(x, y) \leq \text{min\_dist} \\
e^{-(\sigma(x, y) - \text{min\_dist})} & \text{otherwise}
\end{cases}
```

In code:
```julia
Ïˆ(x, y, Ïƒ, min_dist) = Ïƒ(x, y) â‰¤ min_dist ? 1 : exp(-(Ïƒ(x, y) - min_dist))
```

## Optimization Algorithm

Optimizing the embedding is accomplished by stochastic gradient descent, where:
- `fs_set` is the set of $\ell$-simplices (typically 1-simplices)
- `Y_emb` is the target embedding of the points that make up the vertices of `fs_set`
- $\sigma$ is a differentiable distance measure between points in `Y_emb`
- $\phi$ is the differentiable approximation to the fuzzy set membership function for the simplices in the target embedding

The algorithm proceeds as follows:

```julia
function optimize_embedding(fs_set, Y_emb, Ïƒ, Ï•, n_epochs, n_neg_samples)
    Î· = 1  # learning rate
    âˆ‡logÏ•(x, y) = gradient((_x, _y) -> log(Ï•(_x, _y, Ïƒ)), x, y)
    âˆ‡log1_Ï•(x, y) = gradient((_x, _y) -> log(1 - Ï•(_x, _y, Ïƒ)), x, y)

    for e in 1:n_epochs
        for (a, b, p) in fs_setâ‚  # iterate over 1-simplices
            if rand() â‰¤ p  # sample with probability p = Î¼(a)
                # Attractive force (positive sample)
                âˆ‚a, âˆ‚b = Î· * âˆ‡logÏ•(Y_emb[a], Y_emb[b])
                Y_emb[a] -= âˆ‚a

                # Repulsive forces (negative samples)
                for _ in 1:n_neg_samples
                    c = sample(Y_emb)
                    âˆ‚a, âˆ‚c = Î· * âˆ‡log1_Ï•(Y_emb[a], Y_emb[c])
                    Y_emb[a] -= âˆ‚a
                end
            end
        end
        Î· = 1 - e/n_epochs  # linear learning rate decay
    end

    return Y_emb
end
```

The algorithm iterates over edges in the fuzzy simplicial set, sampling each edge with probability equal to its membership strength $\mu(a)$. For each sampled edge:

1. **Attractive force**: Apply gradient descent on $\log\phi(x, y)$ to pull connected points together
2. **Repulsive forces**: Sample `n_neg_samples` random points and apply gradient descent on $\log(1 - \phi(x, y))$ to push disconnected points apart

The learning rate $\eta$ decays linearly from 1 to 0 over the course of training.
